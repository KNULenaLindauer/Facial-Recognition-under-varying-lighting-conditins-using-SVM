{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Detailed Analysis od SVM Model"
      ],
      "metadata": {
        "id": "qeLvuG8c0KTJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note: needs to be executed after SVM_Model.ipynb and in the same environment"
      ],
      "metadata": {
        "id": "Eq7Xcq7FWCUq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IMRZEUt8P6OX"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, classification_report, confusion_matrix\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "import time\n",
        "import psutil"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to evaluate extended performance metrics\n",
        "def evaluate_extended_performance(model, X_train, y_train, X_test, y_test):\n",
        "    # Generate predictions for the test set\n",
        "    y_pred = model.predict(X_test)\n",
        "\n",
        "    # Calculate accuracy\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
        "\n",
        "    # Calculate weighted F1-Score\n",
        "    f1 = f1_score(y_test, y_pred, average='weighted')\n",
        "    print(f\"F1-Score: {f1:.2f}\")\n",
        "\n",
        "    # Calculate precision and recall\n",
        "    precision = precision_score(y_test, y_pred, average='weighted')\n",
        "    recall = recall_score(y_test, y_pred, average='weighted')\n",
        "    print(f\"Precision: {precision:.2f}\")\n",
        "    print(f\"Recall: {recall:.2f}\")\n",
        "\n",
        "    # Perform cross-validation for accuracy (optional, for more reliable performance estimation)\n",
        "    cross_val_score_result = cross_val_score(model, X_train, y_train, cv=5, scoring='accuracy')\n",
        "    print(f\"Cross-Validation Accuracy: {cross_val_score_result.mean() * 100:.2f}%\")\n",
        "\n",
        "    return accuracy, f1, precision, recall, cross_val_score_result.mean()\n",
        "\n",
        "# Example usage of the extended evaluation function\n",
        "accuracy, f1, precision, recall, cross_val_accuracy = evaluate_extended_performance(svm_model, X_train, y_train, X_test, y_test)\n"
      ],
      "metadata": {
        "id": "u-AUigsFXAq7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualization of the extended performance metrics\n",
        "metrics = ['Accuracy', 'F1-Score', 'Precision', 'Recall', 'Cross-Validation Accuracy']\n",
        "values = [accuracy, f1, precision, recall, cross_val_accuracy]\n",
        "\n",
        "# Create a bar plot for the performance metrics\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.barplot(x=metrics, y=values, palette=\"Blues_d\")\n",
        "plt.title(\"Performance Metrics\")\n",
        "plt.ylabel(\"Score\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "T5PzfaUZW2Bt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Optional: Visualize training and prediction time vs memory usage\n",
        "def visualize_performance_metrics(train_time, prediction_time, train_memory_usage, prediction_memory_usage):\n",
        "    # Create subplots for speed and memory usage\n",
        "    fig, ax = plt.subplots(1, 2, figsize=(14, 6))\n",
        "\n",
        "    # Visualization of training and prediction time\n",
        "    sns.barplot(x=[\"Training\", \"Prediction\"], y=[train_time, prediction_time], palette=\"viridis\", ax=ax[0])\n",
        "    ax[0].set_title(\"Speed (Training vs Prediction Time)\")\n",
        "    ax[0].set_ylabel(\"Time (seconds)\")\n",
        "\n",
        "    # Visualization of memory usage\n",
        "    sns.barplot(x=[\"Training\", \"Prediction\"], y=[train_memory_usage, prediction_memory_usage], palette=\"coolwarm\", ax=ax[1])\n",
        "    ax[1].set_title(\"Memory Usage (Training vs Prediction)\")\n",
        "    ax[1].set_ylabel(\"Memory Usage (MB)\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "visualize_performance_metrics(train_time, prediction_time, train_memory_usage, prediction_memory_usage)"
      ],
      "metadata": {
        "id": "nHPdW7JyW2Em"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}